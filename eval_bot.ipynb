{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEN_to_bit_vector(fen):\n",
    "    # Converting the Peices to a bit vector\n",
    "    piece_layer = {\n",
    "        'P': 0,\n",
    "        'N': 1,\n",
    "        'B': 2,\n",
    "        'R': 3,\n",
    "        'Q': 4,\n",
    "        'K': 5,\n",
    "        'p': 6,\n",
    "        'n': 7,\n",
    "        'b': 8,\n",
    "        'r': 9,\n",
    "        'q': 10,\n",
    "        'k': 11\n",
    "    }\n",
    "    \n",
    "    fen = fen.split(' ')\n",
    "    piece_vector = torch.zeros(12, 8, 8)\n",
    "    pieces = fen[0]\n",
    "    rows = pieces.split('/')\n",
    "    for i, row in enumerate(rows):\n",
    "        j = 0\n",
    "        for c in row:\n",
    "            if c.isdigit():\n",
    "                j += int(c)\n",
    "            else:\n",
    "                piece_vector[piece_layer[c], i, j] = 1\n",
    "                j += 1\n",
    "                \n",
    "    # Converting the castling rights to a bit vector\n",
    "    castling_vector = torch.zeros(4)\n",
    "    castling = fen[2]\n",
    "    for c in castling:\n",
    "        if c == 'K':\n",
    "            castling_vector[0] = 1\n",
    "        if c == 'Q':\n",
    "            castling_vector[1] = 1\n",
    "        if c == 'k':\n",
    "            castling_vector[2] = 1\n",
    "        if c == 'q':\n",
    "            castling_vector[3] = 1\n",
    "            \n",
    "    # Converting the en passant square to a bit vector\n",
    "    en_passant_vector = torch.zeros(8)\n",
    "    en_passant = fen[3]\n",
    "    if en_passant != '-':\n",
    "        en_passant = ord(en_passant[0]) - 97\n",
    "        en_passant_vector[en_passant] = 1\n",
    "        \n",
    "    # Getting the current player\n",
    "    curr_player_vector = torch.zeros(8)\n",
    "    curr_player = fen[1]\n",
    "    \n",
    "    if curr_player == 'w':\n",
    "        curr_player_vector = torch.ones(8)\n",
    "        \n",
    "    # Append all the bit vectors\n",
    "    bit_vector = torch.cat((piece_vector.view(-1), castling_vector, en_passant_vector, curr_player_vector))\n",
    "    \n",
    "    return bit_vector\n",
    "    \n",
    "\n",
    "def eval_to_int(eval):\n",
    "    try:\n",
    "        res = int(eval)\n",
    "    except ValueError:\n",
    "        res = 5000 if eval[1] == '+' else -5000\n",
    "        \n",
    "    return torch.tensor(res / 100, dtype=torch.float32)\n",
    "\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, csv):\n",
    "        self.csv = csv\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = FEN_to_bit_vector(self.csv.iloc[idx]['FEN'])\n",
    "        y = eval_to_int(self.csv.iloc[idx]['Evaluation'])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(12 * 8 * 8 + 4 + 8 + 8, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.linear2 = nn.Linear(1024, 2048)\n",
    "        self.bn2 = nn.BatchNorm1d(2048)\n",
    "        self.linear3 = nn.Linear(2048, 4096)\n",
    "        self.bn3 = nn.BatchNorm1d(4096)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.linear4 = nn.Linear(4096, 2048)\n",
    "        self.bn4 = nn.BatchNorm1d(2048)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.linear5 = nn.Linear(2048, 512)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.linear6 = nn.Linear(512, 256)\n",
    "        self.bn6 = nn.BatchNorm1d(256)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.linear7 = nn.Linear(256, 64)\n",
    "        self.bn7 = nn.BatchNorm1d(64)\n",
    "        self.dropout5 = nn.Dropout(0.2)\n",
    "        self.linear8 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.linear1(x)))\n",
    "        x = torch.relu(self.bn2(self.linear2(x)))\n",
    "        x = torch.relu(self.bn3(self.linear3(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn4(self.linear4(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.bn5(self.linear5(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.bn6(self.linear6(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = torch.relu(self.bn7(self.linear7(x)))\n",
    "        x = self.dropout5(x)\n",
    "        x = self.linear8(x)\n",
    "        # Clamp the output to -50 and 50\n",
    "        x = torch.clamp(x, -50, 50)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+667\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/tactic_evals.csv\")\n",
    "\n",
    "row = df.iloc[10]\n",
    "\n",
    "print(row['Evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(700007)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = ChessDataset(df)\n",
    "\n",
    "train_size = int(0.95 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "model = ChessModel().to(device)\n",
    "criterion = nn.SmoothL1Loss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1024]         807,936\n",
      "       BatchNorm1d-2                 [-1, 1024]           2,048\n",
      "            Linear-3                 [-1, 2048]       2,099,200\n",
      "       BatchNorm1d-4                 [-1, 2048]           4,096\n",
      "            Linear-5                 [-1, 4096]       8,392,704\n",
      "       BatchNorm1d-6                 [-1, 4096]           8,192\n",
      "           Dropout-7                 [-1, 4096]               0\n",
      "            Linear-8                 [-1, 2048]       8,390,656\n",
      "       BatchNorm1d-9                 [-1, 2048]           4,096\n",
      "          Dropout-10                 [-1, 2048]               0\n",
      "           Linear-11                  [-1, 512]       1,049,088\n",
      "      BatchNorm1d-12                  [-1, 512]           1,024\n",
      "          Dropout-13                  [-1, 512]               0\n",
      "           Linear-14                  [-1, 256]         131,328\n",
      "      BatchNorm1d-15                  [-1, 256]             512\n",
      "          Dropout-16                  [-1, 256]               0\n",
      "           Linear-17                   [-1, 64]          16,448\n",
      "      BatchNorm1d-18                   [-1, 64]             128\n",
      "          Dropout-19                   [-1, 64]               0\n",
      "           Linear-20                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 20,907,521\n",
      "Trainable params: 20,907,521\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.21\n",
      "Params size (MB): 79.76\n",
      "Estimated Total Size (MB): 79.97\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (12 * 8 * 8 + 4 + 8 + 8,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rafay Khan\\AppData\\Local\\Temp\\ipykernel_12640\\1169881174.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"model_epoch_{start_epoch}.pth\"))\n",
      "Epoch 16: 100%|██████████| 78026/78026 [54:32<00:00, 23.85it/s, loss=5.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, loss: 5.388063728042393\n",
      "Epoch: 16, Validation loss: 5.607604257782388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 78026/78026 [55:32<00:00, 23.42it/s, loss=5.44]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, loss: 5.347583203232866\n",
      "Epoch: 17, Validation loss: 5.503602239056344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 78026/78026 [42:09<00:00, 30.85it/s, loss=5.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, loss: 5.290565274596426\n",
      "Epoch: 18, Validation loss: 5.482320652003413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 78026/78026 [35:52<00:00, 36.24it/s, loss=5.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, loss: 5.234864099939574\n",
      "Epoch: 19, Validation loss: 5.392862740937654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20:  41%|████      | 32173/78026 [15:42<21:25, 35.68it/s, loss=5.16]  "
     ]
    }
   ],
   "source": [
    "loss_epoch = 0.0\n",
    "start_epoch = 15\n",
    "if start_epoch != 0:\n",
    "    model.load_state_dict(torch.load(f\"model_epoch_{start_epoch}.pth\"))\n",
    "# Train the model\n",
    "for epoch in range(start_epoch, 25):\n",
    "    if epoch != start_epoch:\n",
    "        torch.save(model.state_dict(), f\"model_epoch_{epoch}.pth\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    p_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    p_bar.set_description(f\"Epoch {epoch + 1}\")\n",
    "    exponential_moving_loss = loss_epoch\n",
    "    for i, data in p_bar:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # Take exponential moving average of loss and print to p_bar\n",
    "        exponential_moving_loss = 0.99 * exponential_moving_loss + 0.01 * loss.item()\n",
    "        p_bar.set_postfix({'loss': exponential_moving_loss})\n",
    "    loss_epoch = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}, loss: {loss_epoch}\")\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.view(-1, 1))\n",
    "            running_loss += loss.item()\n",
    "    print(f\"Epoch: {epoch + 1}, Validation loss: {running_loss / len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4107/4107 [00:48<00:00, 84.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 5.558965100753269\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "running_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(val_loader), 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        running_loss += loss.item()\n",
    "print(f\"Validation loss: {running_loss / len(val_loader)}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"models/model5/model.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Rafay Khan\\AppData\\Local\\Temp\\ipykernel_17700\\4213666757.py:4: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  state_dict = torch.load(\"models\\model4\\model.pth\")\n",
      "C:\\Users\\Rafay Khan\\AppData\\Local\\Temp\\ipykernel_17700\\4213666757.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"models\\model4\\model.pth\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x788 and 781x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add an extra dimension\u001b[39;00m\n\u001b[0;32m     21\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 22\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_pred\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rafay Khan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rafay Khan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m, in \u001b[0;36mChessModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 26\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x)))\n\u001b[0;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear3(x)))\n",
      "File \u001b[1;32mc:\\Users\\Rafay Khan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rafay Khan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Rafay Khan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x788 and 781x512)"
     ]
    }
   ],
   "source": [
    "# import chess\n",
    "\n",
    "model = ChessModel().to(device)\n",
    "state_dict = torch.load(\"models\\model4\\model.pth\")\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Create new OrderedDict that does not contain `module.`\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] if k.startswith(\"module.\") else k  # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "# Show a handful of predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(70, 100):\n",
    "        x, y = dataset[i]\n",
    "        x = x.to(device).unsqueeze(0)  # Add an extra dimension\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "        print(f\"Prediction: {y_pred.item()}, Actual: {y.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the occurance of some evaluations\n",
    "evals = df['Evaluation'][1:1000]\n",
    "evals = evals.apply(eval_to_int)\n",
    "evals = evals.tolist()\n",
    "evals = [int(e) for e in evals]\n",
    "evals = torch.tensor(evals)\n",
    "unique, counts = evals.unique(return_counts=True)\n",
    "\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"{u.item()}: {c.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(unique, counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
